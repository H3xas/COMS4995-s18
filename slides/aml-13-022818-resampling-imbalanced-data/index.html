<!DOCTYPE html>
<html>
  <head>
    <title>Title</title>
    <meta charset="utf-8">
    <style>
      @import url(https://fonts.googleapis.com/css?family=Garamond);
      @import url(https://fonts.googleapis.com/css?family=Muli:400,700,400italic);
      @import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic);
      body {
        font-family: 'Muli';
        font-size: 140%;
      }
      h1, h2 {
        font-family: 'Garamond';
        font-weight: normal;
        margin-top: 10px;
        margin-bottom: 10px;
      }
      .remark-slide-content h1 {
        font-size: 70px;
        text-align: center;
      }
      .remark-slide-content p, .remark-slide-content li {
        font-size:30px;
        line-height: 1.4;
      }
      .remark-code {
        font-size:30px;
      }
      .remark-slide-content p {
          margin: 5px;
      }
      .remark-slide-container .spacious p,
      .remark-slide-container .spacious li{
          margin-bottom: 50px;
          margin-top: 50px;
      }
      .remark-slide-container .spacious h1{
          margin-bottom: 50px;
      }
      .remark-slide-container .some-space p,
      .remark-slide-container .some-space li,
      .remark-slide-container .some-space h1{
          margin-bottom: 30px;
      }
      .reset-column {
          overflow: auto;
          width: 100%;
      }
      .remark-slide-content .compact p, .remark-slide-content .compact li, .remark-slide-content .compact pre, .remark-slide-content .compact .MathJax_Display{
          font-size: 30px;
          line-height: 1.1;
          display: block;
          margin: 2px 0;
      }
      .padding-top {
          padding-top: 100px;
      }
      .remark-slide-content .smaller p, .remark-slide-content .smaller li,
      .remark-slide-content .smaller .remark-code, .remark-slide-content .smaller a{
          font-size: 25px;
      }
      .normal {
          font-size: 30px;
      }
      .quote_author {
          display: block;
          text-align: right;
          margin-top: 20px;
          font-size: 30px;
          font-family: 'Garamond';
      }
      .larger, .larger .remark-code {
          font-size: 40px;
      }
      .largest, .largest .remark-code {
          font-size: 50px;
      }
      .left-column, .right-column {
          width: 48%;
      }
      .right-column{
          float: right;
      }
      .left-column{
          float: left;
      }
      .narrow-right-column {
          float: right;
          width: 32%
      }
      .wide-left-column {
          float: left;
          width: 65%
      }
      .invisible {
          visibility: hidden
      }
      .remark-code, .remark-inline-code { font-family: 'Ubuntu Mono'; }
    </style>
  </head>
  <body>
    <textarea id="source">

class: center, middle

### W4995 Applied Machine Learning

# Working with Imbalanced Data

02/28/18

Andreas C. Müller

???
N/A

---
class: center, middle

## Recap on imbalanced data

???

---
class: spacious

# Two sources of imbalance

- Asymmetric cost
- Asymmetric data

???

---

class: spacious

# Why do we care?

- Why should cost be symmetric?
- Detect rare events

???

---

# Changing Thresholds

.smaller[
```python
# logistic regression on breast cancer, but change threshold
data = load_breast_cancer()
X_train, X_test, y_train, y_test = train_test_split(data.data, data.target,
                                    stratify=data.target, random_state=0)
lr = LogisticRegression().fit(X_train, y_train)
y_pred = lr.predict(X_test)
print(classification_report(y_test, y_pred))
```
.center[
![:scale 25%](images/precision_recall_breast_cancer_1.png)
]

```python
y_pred = lr.predict_proba(X_test)[:, 1] > .85
print(classification_report(y_test, y_pred))
```
.center[
![:scale 25%](images/precision_recall_breast_cancer_2.png)
]
]

???

---

# Roc Curve

.center[
![:scale 85%](images/roc_svc_rf_curve.png)
]

???

---

class: center, middle

## Remedies for the model

???

---
# Mammography Data

.smaller[
.wide-left-column[
```python
import openml
# mammography dataset https://www.openml.org/d/310
data = openml.datasets.get_dataset(310)
X, y = data.get_data(target=
data.default_target_attribute)
X.shape
```
(11183, 6)
```python
np.bincount(y)
```
array([10923,   260])
]
.narrow-right-column[
.center[
![:scale 130%](images/mammography_data.png)
]
]
]
???
Note: Split one slide to two due to size constraints.

---

# Mammography Data

.smaller[
```python
from sklearn.linear_model import LogisticRegression
scores = cross_val_score(LogisticRegression(), X_train, y_train, cv=10,
                          scoring='roc_auc')
print(scores.mean())
```
0.920
```python
from sklearn.ensemble import RandomForestClassifier
scores = cross_val_score(RandomForestClassifier(n_estimators=100),
                         X_train, y_train, cv=10, scoring='roc_auc')
print(np.mean(scores))
```
0.929
]

???

Note: Split one slide to two due to size constraints.

---

# Mammography Data

.center[
![:scale 85%](images/mammography_data_2.png)
]

???

---

# Basic Approaches

.left-column[
.center[
![:scale 85%](images/basic_approaches.png)
]
]

.right-column[
</br>
Change the training procedure
]

???

---


# Sckit-learn vs resampling

- The transform method only transforms X
- Pipelines work by chaining transforms
- To resample the data, we need to also change y
- Imbalance-learn extends scikit-learn interface with a
“sample” method.
- Imbalance-learn has a custom pipeline that allows
resampling.
- Imbalance-learn: resampling is only performed during fitting
- Warning: not everything in imbalance-learn is multiclass!

???

---

# Random Undersampling

.smaller[
- Drop data from the majority class randomly
- Often untill balanced
- Very fast training (data shrinks to 2x minority)
- Loses data !
```python
from imblearn.under_sampling import RandomUnderSampler
rus = RandomUnderSampler(replacement=False)
X_train_subsample, y_train_subsample = rus.fit_sample(X_train, y_train)
print(X_train.shape)
print(X_train_subsample.shape)
print(np.bincount(y_train_subsample))
```
(8387, 6) </br>
(390, 6) </br>
[195 195]
]

???


---

# Random Undersampling

.smaller[
```python
from imblearn.pipeline import make_pipeline as make_imb_pipeline
undersample_pipe = make_imb_pipeline(RandomUnderSampler(), LogisticRegressionCV())
scores = cross_val_score(undersample_pipe, X_train, y_train,
cv=10, scoring='roc_auc')
print(np.mean(scores))
```
0.917
```python
undersample_pipe_rf = make_imb_pipeline(RandomUnderSampler(),
                                          RandomForestClassifier())
scores = cross_val_score(undersample_pipe_rf, X_train, y_train, cv=10,
                            scoring='roc_auc')
print(np.mean(scores))
```
0.943

]

???

- As accurate with fraction of samples!
- Really good for large datasets

---

# Random Oversmapling

.smaller[
- Repeat samples from the minority class randomly
- Often untill balanced
- Much slower (dataset grows to 2x majority)
```python
from imblearn.over_sampling import RandomOverSampler
ros = RandomOverSampler()
X_train_oversample, y_train_oversample = ros.fit_sample(X_train, y_train)
print(X_train.shape)
print(X_train_oversample.shape)
print(np.bincount(y_train_oversample))
```
(8387, 6) </br>
(16384, 6) </br>
[8192 8192]
]
???

---

# Random Oversampling

.smaller[
```python
oversample_pipe = make_imb_pipeline(RandomOverSampler(), LogisticRegression())
scores = cross_val_score(oversample_pipe, X_train, y_train,
cv=10, scoring='roc_auc')
print(np.mean(scores))
```
0.918
```python
oversample_pipe_rf = make_imb_pipeline(RandomOverSampler(),
RandomForestClassifier())
scores = cross_val_score(oversample_pipe_rf, X_train, y_train,
cv=10, scoring='roc_auc')
print(np.mean(scores))
```
0.916

</br>
Logreg the same, Random Forest much worse than before
]

???

---

# ROC Curves for LogReg

.center[
![:scale 75%](images/roc_curves_for_logreg.png)
]

???

---

# ROC Curves for Random Forest

.center[
![:scale 75%](images/roc_curves_for_rf.png)
]

???

---
class: spacious

# Class-weights

- Instead of repeating samples, re-weight the loss function.
- Works for most models!
- Same effect as over-sampling (though not random), but not as expensive (dataset size the same).

???

---
class: spacious

# Class-weights in linear models

`$$\min_{w \in ℝ^{p}}-C \sum_{i=1}^n\log(\exp(-y_iw^T \textbf{x}_i) + 1) + ||w||_2^2$$`

`$$ \min_{w \in \mathbb{R}^p} -\sum_{i=1}^n C_{y_i} \log(\exp(-y_i w^T \mathbf{x}_i) + 1) + ||w||^2_2 $$`

Similar for linear and non-linear SVM

???

---

# Class weights in trees

???

---

#Using Class-Weights

.smaller[
```python
from sklearn.linear_model import LogisticRegression
scores = cross_val_score(LogisticRegression(class_weight='balanced'),
                         X_train, y_train, cv=10, scoring='roc_auc')
print(scores.mean())
```
0.918
```python
from sklearn.ensemble import RandomForestClassifier
scores = cross_val_score(RandomForestClassifier(n_estimators=100,
class_weight='balanced'),X_train, y_train, cv=10, scoring='roc_auc')
print(np.mean(scores))
```
0.917
]

???

---
class: spacious

# Ensemble Resampling

- Random resampling separate for each instance in
an ensemble!
- Paper: “Exploratory Undersampling for Class Imbalance Learning”
- Not in sklearn (yet), not totally easy with
imbalance-learn (but soon).

???

---

# Quick & Dirty Easy Ensemble

.smaller[
```python
from sklearn.base import clone

def make_resampled_ensemble(estimator, n_estimators=100):
    estimators = []
    for i in range(n_estimators):
        est = clone(estimator)
        if hasattr(est, "random_state"):
            est.random_state = i
        pipe = make_imb_pipeline(RandomUnderSampler(random_state=i,
                                  replacement=True),est)
        estimators.append(("est_i".format(i), pipe))
    return VotingClassifier(estimators, voting="soft")

resampled_tree_test = make_resampled_ensemble(DecisionTreeClassifier
                                              (max_features='auto'))
scores = cross_val_score(resampled_tree_test, X_train, y_train, cv=10,
                                                scoring='roc_auc')
print(np.mean(scores))
```
0.960


]

???

-As cheap as undersampling, but much better results than anything else! </br>
-Didn't do anything for Logistic Regression.

---
class: center, middle

## Smart resampling
## (based on nearest neighbour heuristics from the 70's)

???

---
class: spacious

# Edited Nearest Neighbours

- Originally as heuristic for reducing dataset for KNN
- Remove all samples that are misclassified by KNN
from training data (mode) or that have any point
from other class as neighbor (all).
- “Cleans up” outliers and boundaries.

???

---

.center[
![:scale 90%](images/edited_nearest_neighbour.png)
]

???

---

# Edited Nearest Neighbours

.smaller[
```python
from imblearn.under_sampling import EditedNearestNeighbours
enn = EditedNearestNeighbours(n_neighbors=5)
X_train_enn, y_train_enn = enn.fit_sample(X_train, y_train)

enn_mode = EditedNearestNeighbours(kind_sel="mode", n_neighbors=5)
X_train_enn_mode, y_train_enn_mode = enn_mode.fit_sample(X_train, y_train)
```
.center[
![:scale 100%](images/edited_nearest_neighbour_2.png)
]
]

???

---

.smaller[
```python
enn_pipe = make_imb_pipeline(EditedNearestNeighbours(n_neighbors=5),
                             LogisticRegression())
scores = cross_val_score(enn_pipe, X_train, y_train, cv=10, scoring='roc_auc')
print(np.mean(scores))
```
0.920
```python
enn_pipe_rf = make_imb_pipeline(EditedNearestNeighbours(n_neighbors= 5),
                                  RandomForestClassifier(n_estimators=100))
scores = cross_val_score(enn_pipe_rf, X_train, y_train, cv=10, scoring='roc_auc')
print(np.mean(scores))
```
0.944
]
???

---

# Condensed Nearest Neigbours

- Iteratively adds points to the data that are
misclassified by KNN
- Focuses on the boundaries
- Usually removes many

```python
cnn = CondensedNearestNeighbour()
X_train_cnn, y_train_cnn = cnn.fit_sample(X_train, y_train)
print(X_train_cnn.shape)
print(np.bincount(y_train_cnn))
```
(556, 6) </br>
[361 195]

???

---

.center[
![:scale 100%](images/edited_condensed_nn.png)
]

???

---

.smaller[
```python
cnn_pipe = make_imb_pipeline(CondensedNearestNeighbour(), LogisticRegression())
scores = cross_val_score(cnn_pipe, X_train, y_train, cv=10, scoring='roc_auc')
print(np.mean(scores))
```
0.919
```python
cnn_pipe = make_imb_pipeline(CondensedNearestNeighbour(),
                              RandomForestClassifier(n_estimators=100))
scores = cross_val_score(cnn_pipe, X_train, y_train, cv=10, scoring='roc_auc')
print(np.mean(scores))
```
0.948
]
???

---
class: center, middle

## Synthetic Sample Generation

???

---

# Synthetic Minority Oversampling Technique
# (SMOTE)

- Adds synthetic interpolated data to smaller class
- For each sample in minority class:</br>
  – Pick random neighbor from k neighbors.</br>
  – Pick point on line connecting the two uniformly</br>
  – Repeat.</br>
- Leads to very large datasets (oversampling)
- Can be combined with undersampling strategies


???

---

.center[
![:scale 100%](images/smote.png)
]

???

---

class: center, middle

.center[
![:scale 100%](images/smote_3.png)
]


???

---

.smaller[
```python
smote_pipe = make_imb_pipeline(SMOTE(), LogisticRegression())
scores = cross_val_score(smote_pipe, X_train, y_train, cv=10, scoring='roc_auc')
print(np.mean(scores))
```
0.919
```python
smote_pipe_rf = make_imb_pipeline(SMOTE(),RandomForestClassifier(n_estimators=100))
scores = cross_val_score(smote_pipe_rf,X_train,y_train, cv=10, scoring='roc_auc')
print(np.mean(scores))
```
0.947
```python
param_grid = {'smote__k_neighbors': [3, 5, 7, 9, 11, 15, 31]}
search = GridSearchCV(smote_pipe_rf, param_grid, cv=10, scoring="roc_auc")
search.fit(X_train, y_train)
```
]

???

---

.center[
![:scale 90%](images/param_smote_k_neighbours.png)
]

???

---


.center[
![:scale 100%](images/smote_k_neighbours.png)
]

???

---
class: spacious

# Summary

- Always check roc_auc, look at curve
- Undersampling is very fast and can help!
- Undersampling + Ensembles is very powerful!
- Many smart sampling strategies, mixed outcomes
- SMOTE allows adding new interpolated samples,
works well in practice
- More advanced variants of SMOTE available

???



    </textarea>
    <script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

    <script>
    // Config Remark
    remark.macros['scale'] = function (percentage) {
        var url = this;
        return '<img src="' + url + '" style="width: ' + percentage + '" />';
    };
    config_remark = {
        highlightStyle: 'magula',
        highlightSpans: true,
        highlightLines: true,
        ratio: "16:9"
    };
      var slideshow = remark.create(config_remark);
    // Configure MathJax
    MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] /* removed 'code' entry*/
    }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i = 0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
    </script>
  </body>
</html>
