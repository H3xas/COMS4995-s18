<!DOCTYPE html>
<html>
  <head>
    <title>Title</title>
    <meta charset="utf-8">
    <style>
      @import url(https://fonts.googleapis.com/css?family=Garamond);
      @import url(https://fonts.googleapis.com/css?family=Muli:400,700,400italic);
      @import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic);
      body {
        font-family: 'Muli';
        font-size: 140%;
      }
      h1, h2 {
        font-family: 'Garamond';
        font-weight: normal;
        margin-top: 10px;
        margin-bottom: 10px;
      }
      .remark-slide-content h1 {
        font-size: 70px;
        text-align: center;
      }
      .remark-slide-content p, .remark-slide-content li {
        font-size:30px;
        line-height: 1.4;
      }
      
      .remark-code {
        font-size:30px;
      }
      .remark-slide-content p {
          margin: 5px;
      }
      .remark-slide-container .spacious p,
      .remark-slide-container .spacious li{
          margin-bottom: 50px;
          margin-top: 50px;
      }
      .remark-slide-container .spacious h1{
          margin-bottom: 50px;
      }
      .remark-slide-container .some-space p,
      .remark-slide-container .some-space li,
      .remark-slide-container .some-space h1{
          margin-bottom: 30px;
      }
      .reset-column {
          overflow: auto;
          width: 100%;
      }
      .remark-slide-content .compact p, .remark-slide-content .compact li, .remark-slide-content .compact pre, .remark-slide-content .compact .MathJax_Display{
          font-size: 30px;
          line-height: 1.1;
          display: block;
          margin: 2px 0;
      }
      .padding-top {
          padding-top: 100px;
      }
      .remark-slide-content .smaller p, .remark-slide-content .smaller li,
      .remark-slide-content .smaller .remark-code, .remark-slide-content .smaller a{
          font-size: 25px;
      }

      .normal {
          font-size: 30px;
      }
      .quote_author {
          display: block;
          text-align: right;
          margin-top: 20px;
          font-size: 30px;
          font-family: 'Garamond';
      }
      .larger, .larger .remark-code {
          font-size: 40px;
      }
      .largest, .largest .remark-code {
          font-size: 50px;
      }
      .left-column, .right-column {
          width: 48%;
      }
      .right-column{
          float: right;
      }
      .left-column{
          float: left;
      }
      .narrow-right-column {
          float: right;
          width: 32%
      }
      .wide-left-column {
          float: left;
          width: 65%
      }
      .invisible {
          visibility: hidden
      }
      .remark-code, .remark-inline-code { font-family: 'Ubuntu Mono'; }
      .column:first-of-type {float:left}
      .column:last-of-type {float:right}
      .tiny-code .remark-code, .remark-inline-code .tiny-code{
        font-size: 15px;
      }
      .split-40 .column:first-of-type {width: 50%}
      .split-40 .column:last-of-type {width: 50%}
    </style>
  </head>
  <body>
    <textarea id="source">

class: center, middle

### W4995 Applied Machine Learning

# PCA, Discriminants, Manifold Learning

03/05/18

Andreas C. Müller

???
---
class: centre,middle
# Principal Component Analysis
???
---
.center[
![:scale 70%](images/pca-intuition.png)
]

???
---
# PCA objective(s)
Restricted rank reconstruction
`$$\large\text{min}_{X', \text{rank}(X') = r}\|X-X'\|$$`
.center[
![:scale 45%](images/pca-intuition.png)
]
???
---
class:split-40
# PCA objective(s)
Find directions of maximum variance.
.left-column[
`$$\large\max\limits_{u_1 \in R^p, \| u_1 \| = 1} \text{var}(Xu_1)$$`
`$$\large\max\limits_{u_1 \in R^p, \| u_1 \| = 1} u_1^T \text{cov} (X) u_1$$`
Subtract projection onto u1, iterate to find more components. 
<br />
<br />
Only well-defined up to sign / direction of arrow!]
.smaller.right-column[
Find projection (onto one vector) that maximizes the variance observed in the data.
.center[
![:scale 90%](images/pca-intuition.png)
]
]
???
---
class: spacious
# PCA Computation
Center X (subtract mean).
<br />
In practice: Also scale to unit variance. 
<br />
Compute singular value decomposition:
![:scale 90%](images/pca-computation.png)
???
---
class: center, spacious
# Back to intuition
![:scale 55%](images/pca-intuition.png)
???
---
class: center, spacious
# Whitening
![:scale 80%](images/whitening.png)
<br />
Same as using PCA without whitening, then doing StandardScaler.
???
---
class: split-40
# PCA for Visualization
.tiny-code.left-column[```python
from sklearn.decomposition import PCA
print(cancer.data.shape)
pca = PCA(n_components=2)
X_pca = pca.fit_transform(cancer.data)
print(X_pca.shape)
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=cancer.target)
plt.xlabel("first principal component")
plt.ylabel("second principal component")
components = pca.components_
plt.imshow(components.T)
plt.yticks(range(len(cancer.feature_names)), cancer.feature_names)
plt.colorbar()
```]
.right-column[
![:scale 80%](images/pca-for-visualization-cancer-data.png)
<br /> 
![:scale 50%](images/pca-for-visualization-components-color-bar.png)]
???
---
class:spacious
# Scaling!
.tiny-code[```python
pca_scaled = make_pipeline(StandardScaler(), PCA(n_components=2))
X_pca_scaled = pca_scaled.fit_transform(cancer.data)
plt.scatter(X_pca_scaled[:, 0], X_pca_scaled[:, 1], c=cancer.target, alpha=.9)
plt.xlabel("first principal component")
plt.ylabel("second principal component")
```]
.center[![:scale 45%](images/scaled-pca-for-visualization-cancer-data.png)]
<br />
Imagine one feature with very large scale. Without scaling, it’s guaranteed to be the first principal component!
???
---
class:split-40
# Inspecting components
.tiny-code[```python
components = pca_scaled.named_steps['pca'].components_
plt.imshow(components.T)
plt.yticks(range(len(cancer.feature_names)), cancer.feature_names)
plt.colorbar()
```]
.smaller.left-column[
![:scale 60%](images/inspecting-pca-scaled-components.png)
<br />
Direction (sign) of component is meaningless!]

.right-column[
![:scale 100%](images/inspecting-pca-scaled-components-2.png)]

???
---
# PCA for regularization
.tiny-code[
```python
from sklearn.linear_model import LogisticRegression
X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, stratify=cancer.target, random_state=0)
lr = LogisticRegression(C=10000).fit(X_train, y_train)
print(lr.score(X_train, y_train))
print(lr.score(X_test, y_test))
```
```
0.992957746479
0.944055944056
```
```python
pca_lr = make_pipeline(StandardScaler(), PCA(n_components=2), LogisticRegression(C=10000))
pca_lr.fit(X_train, y_train)
print(pca_lr.score(X_train, y_train))
print(pca_lr.score(X_test, y_test))
```
```
0.960093896714
0.923076923077
```
]
???
---
# Variance covered
.center[
![:scale 55%](images/variance-covered.png)
]
.tiny-code[```python
pca_lr = make_pipeline(StandardScaler(), PCA(n_components=6), LogisticRegression(C=10000))
pca_lr.fit(X_train, y_train)
print(pca_lr.score(X_train, y_train))
print(pca_lr.score(X_test, y_test))
```
```
0.981220657277
0.958041958042
```]

???
---
class:split-40
# Interpreting coefficients
.tiny-code[```python
pca = pca_lr.named_steps['pca']
lr = pca_lr.named_steps['logisticregression']
coef_pca = pca.inverse_transform(lr.coef_)```]
<br /> 
Rotating coefficients back into input space. Makes sense because model is linear! Otherwise more tricky.
<br />
<br />
.center[Comparing PCA + Logreg vs plain Logreg:]
<br /> 
.left-column[
![:scale 100%](images/PCA+logreg.png)
]
.right-column[
![:scale 100%](images/logreg+noPCA.png)
]
???
---
class:split-40
# PCA is Unsupervised!
.left-column[
![:scale 100%](images/pca-is-unsupervised-1.png)
]
.right-column[
![:scale 100%](images/pca-is-unsupervised-2.png)
]
Dropping the first two principal components will result in random model!
All information is in the smallest principal component!
???

---
# PCA for feature extraction
.center[
![:scale 85%](images/pca-for-feature-extraction.png)
]
???
---
# 1-NN and Eigenfaces
.tiny-code[
```python
from sklearn.neighbors import KNeighborsClassifier
# split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(
    X_people, y_people, stratify=y_people, random_state=0)
# build a KNeighborsClassifier using one neighbor
knn = KNeighborsClassifier(n_neighbors=1)
knn.fit(X_train, y_train)
print("Test set score of 1-nn: {:.2f}".format(knn.score(X_test, y_test)))
```

```
Test set score of 1-nn: 0.23
```
```python
pca = PCA(n_components=100, whiten=True, random_state=0).fit(X_train)
X_train_pca = pca.transform(X_train)
X_test_pca = pca.transform(X_test)
print("X_train_pca.shape: {}".format(X_train_pca.shape))
```
```
X_train_pca.shape: (1547, 100)
```
```python
knn = KNeighborsClassifier(n_neighbors=1)
knn.fit(X_train_pca, y_train)
print("Test set accuracy: {:.2f}".format(knn.score(X_test_pca, y_test)))
```
```
Test set accuracy: 0.31
```
]
???
---

# Reconstruction
.center[
![:scale 70%](images/reconstruction.png)
]
???
---
class:split-40
# PCA for outlier detection
.tiny-code[```python
pca = PCA(n_components=100).fit(X_train)
reconstruction_errors = np.sum((X_test - pca.inverse_transform(pca.transform(X_test))) ** 2, axis=1)
```
]
.left-column[
![:scale 90%](images/best-reconstructions.png)
<br />
Best reconstructions
]
.right-column[
![:scale 90%](images/worst-reconstructions.png)
<br />
Worst reconstructions
]
???
---
class: centre,middle
# Manifold Learning
???
---
![:scale 80%](images/manifold-learning-structure.png)
<br />
Learn underlying “manifold” structure, use for dimensionality reduction.
???
---
# Pros and Cons
- For visualization only
- Axes don’t correspond to anything in the input space.
- Often can’t transform new data. 
- Pretty pictures!
???
---
# Algorithms in sklearn
- KernelPCA – does PCA, but with kernels! 
<br /> Eigenvalues of kernel-matrix
- Spectral embedding (Laplacian Eigenmaps) 
<br />Uses eigenvalues of graph laplacian
- Locally Linear Embedding
- Isomap “kernel PCA on manifold”
- t-SNE (t-distributed stochastic neighbor embedding)
???
---
# t-SNE
- Starts with a random embedding
- Iteratively updates points to make “close” points close.
- Global distances are less important, neighborhood counts.
- Good for getting coarse view of topology.
- Can be good for  nding interesting data point
???
class:split-40
---
.smaller[```python
from sklearn.manifold import TSNE
from sklearn.datasets import load_digits
digits = load_digits()
X = digits.data / 16.
X_tsne = TSNE().fit_transform(X)
X_pca = PCA(n_components=2).fit_transform(X)
```]
.left-column[![:scale 90%](images/tsne-digits.png)]
.right-column[![:scale 90%](images/pca-digits.png)]
???
---
class:split-40
# Tuning t-SNE
- Important parameter: perplexity
- Intuitively: bandwidth of neighbors to consider 
<br /> 
(low perplexity: only close neighbors)

.left-column[
![:scale 55%](images/tsne-tuning-10.png)
![:scale 55%](images/tsne-tuning-30.png)]
http://distill.pub/2016/misread-tsne/
.right-column[
![:scale 53%](images/tsne-tuning-100.png)
![:scale 53%](images/tsne-tuning-300.png)]

???
---
class:center
![:scale 63%](images/tsne-embeddings-digits.png)
???
---
class:spacious
![:scale 25%](images/tsne-moons.png) ![:scale 55%](images/tsne-perplexity.png)
???
---
class: middle
# Discriminant Analysis
???
---
# Linear Discriminant Analysis aka Fisher Discriminant
- Generative model: assumes each class has Gaussian distribution
- Covariances are the same for all classes.
- Very fast: only compute means and invert covariance matrix (works well if n_features << n_samples)
- Leads to linear decision boundary.
- Imagine: transform space by covariance matrix, then nearest centroid.
- No parameters to tune!
- Don’t confuse with Latent Dirichlet Allocation (LDA)
???
---
# Quadratic Discriminant Analysis
- Each class is Gaussian, but separate covariance matrices!
- More flexible (quadratic decision boundary), but less robust: have less points per covariance matrix.
- Can’t think of it as transformation of the space.
???
---
class:center
![:scale 55%](images/linear-vs-quadratic-discriminant-analysis.png)
???
---
# Discriminants and PCA
- Both fit Gaussian model
- PCA for the whole data
- LDA multiple Gaussians with shared covariance
- Can use LDA to transform space!
- At most as many components as there are classes (needs between class variance)
???
---
class:center
![:scale 100%](images/pca-lda.png)
???
---
class:center
# Data where PCA failed
![:scale 100%](images/pca-fail.png)
???
---
# Summary
- PCA good for visualization, exploring correlations
- PCA can sometimes help with classi cation as regularization or for feature extraction.
- Manifold learning makes nice pictures.
- LDA is a handy supervised alternative to PCA that also yields a rotation of the input space.
???
---
class: center, middle

# Questions ?

    </textarea>
    <script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

    <script>
    // Config Remark
    remark.macros['scale'] = function (percentage) {
        var url = this;
        return '<img src="' + url + '" style="width: ' + percentage + '" />';
    };
    config_remark = {
        highlightStyle: 'magula',
        highlightSpans: true,
        highlightLines: true,
        ratio: "16:9"
    };
      var slideshow = remark.create(config_remark);
    // Configure MathJax
    MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] /* removed 'code' entry*/
    }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i = 0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
    </script>
  </body>
</html>
