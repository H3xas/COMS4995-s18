<!DOCTYPE html>
<html>
  <head>
    <title>Neural Networks</title>
    <meta charset="utf-8">
    <style>
      @import url(https://fonts.googleapis.com/css?family=Garamond);
      @import url(https://fonts.googleapis.com/css?family=Muli:400,700,400italic);
      @import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic);
      body {
        font-family: 'Muli';
        font-size: 140%;
      }
      h1, h2 {
        font-family: 'Garamond';
        font-weight: normal;
        margin-top: 10px;
        margin-bottom: 10px;
      }
      .remark-slide-content h1 {
        font-size: 70px;
        text-align: center;
      }
      .remark-slide-content p, .remark-slide-content li {
        font-size:30px;
        line-height: 1.4;
      }
      .remark-code {
        font-size:30px;
      }
      .remark-slide-content p {
          margin: 5px;
      }
      .remark-slide-container .spacious p,
      .remark-slide-container .spacious li{
          margin-bottom: 50px;
          margin-top: 50px;
      }
      .remark-slide-container .spacious h1{
          margin-bottom: 50px;
      }
      .remark-slide-container .some-space p,
      .remark-slide-container .some-space li,
      .remark-slide-container .some-space h1{
          margin-bottom: 30px;
      }
      .reset-column {
          overflow: auto;
          width: 100%;
      }
      .remark-slide-content .compact p, .remark-slide-content .compact li, .remark-slide-content .compact pre, .remark-slide-content .compact .MathJax_Display{
          font-size: 30px;
          line-height: 1.1;
          display: block;
          margin: 2px 0;
      }
      .padding-top {
          padding-top: 100px;
      }
      .remark-slide-content .smaller p, .remark-slide-content .smaller li,
      .remark-slide-content .smaller .remark-code, .remark-slide-content .smaller a{
          font-size: 25px;
      }
      .normal {
          font-size: 30px;
      }
      .quote_author {
          display: block;
          text-align: right;
          margin-top: 20px;
          font-size: 30px;
          font-family: 'Garamond';
      }
      .larger, .larger .remark-code {
          font-size: 40px;
      }
      .largest, .largest .remark-code {
          font-size: 50px;
      }
      .left-column, .right-column {
          width: 48%;
      }
      .right-column{
          float: right;
      }
      .left-column{
          float: left;
      }
      .narrow-right-column {
          float: right;
          width: 32%
      }
      .wide-left-column {
          float: left;
          width: 65%
      }
      .invisible {
          visibility: hidden
      }
      .remark-code, .remark-inline-code { font-family: 'Ubuntu Mono'; }
    </style>
  </head>
  <body>
    <textarea id="source">

class: center, middle

### W4995 Applied Machine Learning

# Neural Networks

04/16/18

Andreas C. Müller
???
---
# History

- Nearly everything we talk about today existed ~1990

- What changed?

  – More data

  – Faster computers (GPUs)

  – Some improvements:
    - relu
    - Drop-out
    - adam
    - batch-normalization

???
---
#Supervised Neural Networks

- Non-linear models for classification and regression

- Work well for very large datasets

- Non-convex optimization

- Notoriously slow to train – need for GPUs

- Use dot products etc require preprocessing, → similar to
SVM or linear models, unlike trees

- MANY variants (Convolutional nets, Gated Recurrent neural
networks, Long-Short-Term Memory, recursive neural
networks, variational autoencoders, general adverserial
networks, neural turing machines...)

???
---
#Logistic regression drawn as neural net

.center[
![:scale 40%](images/log_reg_nn.png)
]

???
---

#Basic Architecture (for making predictions)

.center[
![:scale 30%](images/nn_basic_arch.png)
]

`$ h(x) = f(Wx+b) $`

`$ o(x) = g(W'h(x) + b') $`
???
 First let’s describe how to make a prediction given a
  model.

 Input denotes single sample, here three input features.
 Hidden layer here 4 units is matrix multiply with W, b
  added (size of b is 4 here), followed by the univariate
  non-linear function f – sigmoid, tanh, rectifying linear
  function.

 Output is a matrix multiplication with different weight
matrix W’, b’ added (size 2 here), followed by
another non-linear function g. The function g for the
output layer is often different: identity for regression,
soft-max for classification.

 We want to learn W (3x4) W’ (4x2), b (4,), b’ (2,).

 Can think of it as logistic regression with learning a
non-linear basis transformation.
---
#Can have arbitrarily many layers

.center[
![:scale 60%](images/nn_manylayers.png)
]
???
- Hidden layers usually all have the same non-linear
  function, weights are different for each layer.

- Many layers → “deep learning”.
- This is called a multilayer perceptron, feed-forward neural network, vanilla feed-forward neural network.

- For regression usually single output neuron with linear activation.
- For classification one-hot-encoding of classes, n_classes many output variables with softmax.
---
#Nonlinear activation function

.center[
![:scale 65%](images/nonlin_fn.png)
]
???

- Choices for activation function f of hidden layers.

- Traditional tanh (or logistic sigmoid, not shown, but similar).

- Tanh squashes to open interval (-1, 1).

- Relu – recent trend, linear function x=y for positive,
constant for negative values. Bias allows shifting the
cut-off (~ linear splines)

---
# Training Objective

`$$ h(x) = f(W_1x+b_1) $$`
`$$ o(x) = g(W_2h(x)+b_2) = g(W_2f(W_1x + b_1) + b_2)$$`
`$$ \min_{W_1,W_2,b_1,b_2} \sum\limits_{i=1}^N l(y_i,o(x_i)) $$` 

- Could add regularization
.center[
`$ \min_{W_1,W_2,b_1,b_2} \sum\limits_{i=1}^N l(y_i,g(W_2h(x)+b_2) = g(W_2f(W_1x + b_1) + b_2)) $`
]
- `$l$` Squared loss for regression. Cross-entropy loss (multi-class log-loss) for classification

???
---

#Backpropogation

- For gradient based-method need `$ \frac{\partial o(x)}{\partial W_i} $` and `$\frac{\partial o(x)}{\partial b_i}$`

`$$ net(x) := W_1x + b_1 $$`

.center[![:scale 60%](images/backprop_eqn.png)]

Backpropogation = Chain Rule + Dynamic Programming

???
- Easy to write down for last layer

- Example for squared loss (g is identity for
regression)

- Can use the chain rule to compute other gradients

- Bottom layers require partial derivatives of upper
layers → reuse results

- Backpropagation: dynamic programming + chain rule

- “backward pass” compute partial derivatives starting at the last layer.

- You should try to go through that yourself once


---
#But wait!

.center[
![:scale 75%](images/relu_differentiability.png)
]

???
- relu is not differentiable.

- But it’s differentiable almost anywhere.

- “subgradient descent” - little issues in practice

---
#Optimizing W, b

- Batch
`$ W_i \leftarrow W_i - \eta\sum\limits_{j=1}^N \frac{l(x_j,y_j)}{W_i} $`

- Minibatch
`$ W_i \leftarrow W_i - \eta\sum\limits_{j=k}^{k+m} \frac{l(x_j,y_j)}{W_i}$`

- Online/Stochastic
`$ W_i \leftarrow W_i - \eta\frac{l(x_j,y_j)}{W_i}$`

???
- Standard solvers: l-bfgs, newton, cg

- Problem: Hessian too expensive, can to l-bfgs

- Computing gradients over whole dataset expensive

- Stochastic Gradient Descent to rescue

- Actually use mini-batches
---
class:spacious
# Learning Heuristics

- Constant `$\eta$` not good

- Can decrease `$\eta$`

- Better: adaptive `$\eta$` for each entry if W_i

- State-of-the-art: adam (with magic numbers)

- https://arxiv.org/pdf/1412.6980.pdf

- http://sebastianruder.com/optimizing-gradient-descent/
???
---
class:spacious
# Picking Optimization Algorithms

- Small dataset: off the shelf like l-bfgs

- Big dataset: adam

- Have time &amp; nerve: tune the schedule

???
---
#Neural Nets with sklearn

.center[
![:scale 45%](images/nn_sklearn.png)
]

.smaller[
```python
mlp = MLPClassifier(solver='lbfgs', random_state=0).fit(X_train, y_train)
print(mlp.score(X_train, y_train))
print(mlp.score(X_test, y_test))
```
```
1.0
0.88
```
]

???
- Don’t user sklearn for anything but toy problems in
neural nets.
---
#Random State

.center[
![:scale 75%](images/random_state.png)
]

???
This net is also way over capacity and can overfit in many ways.
Regularization might make it less dependent on initialization.
---
# Hidden Layer Size

.smaller[
```python
mlp = MLPClassifier(solver='lbfgs', hidden_layer_size=(5,),random_state=10).
                    fit(X_train, y_train)
print(mlp.score(X_train, y_train))
print(mlp.score(X_test, y_test))
```
```
0.9333333333333
0.82
```
]
.center[
![:scale 50%](images/hidden_layer_size.png)
]


???
---

# Hidden Layer Size

.smaller[
```python
mlp = MLPClassifier(solver='lbfgs', hidden_layer_sizes=(10, 10, 10), random_state=0
                   )
mlp.fit(X_train, y_train)
print(mlp.score(X_train, y_train))
print(mlp.score(X_test, y_test))
```
```
0.973333333333
0.84
```
]
.center[
![:scale 48%](images/hidden_layer_size_2.png)
]

???
---
#Activation Functions
.smaller[
```python
mlp = MLPClassifier(solver='lbfgs', hidden_layer_sizes=(10, 10, 10),
                    activation='tanh', random_state=0)
mlp.fit(X_train, y_train)
print(mlp.score(X_train, y_train))
print(mlp.score(X_test, y_test))
```
```
1.0
0.92
```
]

.center[
![:scale 45%](images/activation_functions_plot.png)
]

???
- Using tanh we get smoother boundaries

- Here actually it fits the data better.

- It might be that relu doesn’t work that well with l-bfgs or
with using these very small hidden layer sizes.

- For large networks, relu is definitely preferred.
---
# Regression

.center[
![:scale 55%](images/regression_plot.png)
]

.smaller[
```python
from sklearn.neural_network import MLPRegressor
mlp_relu = MLPRegressor(solver="lbfgs").fit(X, y)
mlp_tanh = MLPRegressor(solver="lbfgs", activation='tanh').fit(X, y)
```]

???
---
class:spacious
# Complexity Control

- Number of parameters

- Regularization

- Early Stopping

- (drop-out)

???
---
#Grid-Searching Neural Nets

.smaller[
```python
from sklearn.datasets import load_breast_cancer
data = load_breast_cancer()
X_train, X_test, y_train, y_test = train_test_split(
    data.data, data.target, stratify=data.target, random_state=0)
scaler = StandardScaler().fit(X_train)
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)
```]

.smaller[
```python
from sklearn.model_selection import GridSearchCV
pipe = make_pipeline(StandardScaler(), MLPClassifier(solver="lbfgs", 
                     random_state=1))
param_grid = {'mlpclassifier__alpha': np.logspace(-3, 3, 7)}
grid = GridSearchCV(pipe, param_grid, cv=5)
```]

.smaller[
```python
results = pd.DataFrame(grid.cv_results_)
res = results.pivot_table(index="param_mlpclassifier__alpha",
                          values=["mean_test_score", "mean_train_score"])
res
```]
???
---
.center[![:scale 50%](images/gridsearch_table.png)]

.center[![:scale 45%](images/gridsearch_plot.png)]

???
---
#Searching hidden layer sizes
.smaller[
```python
from sklearn.model_selection import GridSearchCV
pipe = make_pipeline(StandardScaler(), MLPClassifier(solver="lbfgs"
                     ,random_state=1))
param_grid = {'mlpclassifier__hidden_layer_sizes':
              [(10,), (50,), (100,), (500,), (10, 10), (50, 50), (100, 100), (500, 
              500)]
             }
grid = GridSearchCV(pipe, param_grid, cv=5)
grid.fit(X_train, y_train)
```]
.center[![:scale 40%](images/search_hidden_layers_plot.png)]

???
---
class:center,middle
#Getting Flexible and Scaling Up

???
Now we'll look at neural nets beyond
scikit-learn.
There are two main motivations for this: scaling to
larger datasets and larger networks using GPUs, and
having more flexible ways to create the neural
network architectures.
If you want to get serious about neural nets, you are
likely to include some modifications to the standard
network. So how could you go about doing that?
---
class:spacious
# Write your own neural networks
.smaller[
```python
class NeuralNetwork(object):
    def __init__(self):
        # initialize coefficients and biases
        pass
    def forward(self, x):
        activation = x
        for coef, bias in zip(self.coef_, self.bias_):
            activation = self.nonlinearity(np.dot(activation, coef) + bias)
        return activation
    def backward(self, x):
        # compute gradient of stuff in forward pass
        pass
```]
???
It’s actually pretty straight-forward to write the
prediction functions, or forward pass of a neural
network. There is matrix multiplications and
nonlinearities. It quickly becomes more complicated
when you use some of the tricks we’ll discuss today,
but even for this writing down the gradient can be a
bit tricky and something people used to get wrong all
the time.
So they came up with a trick that avoided having to
write down the gradients yourself, also known as the
backward pass.
That trick is autodiff.
---
#Autodiff

.smaller[
```python
# http://mxnet.io/architecture/program_model.html
class array(object) :
    """Simple Array object that support autodiff."""
    def __init__(self, value, name=None):
        self.value = value
        if name:
            self.grad = lambda g : {name : g}
    def __add__(self, other):
        assert isinstance(other, int)
        ret = array(self.value + other)
        ret.grad = lambda g : self.grad(g)
        return ret
    def __mul__(self, other):
        assert isinstance(other, array)
        ret = array(self.value * other.value)
        def grad(g):
            x = self.grad(g * other.value)
            x.update(other.grad(g * self.value))
            return x
        ret.grad = grad
        return ret
```]

???
Here is a toy implementation of the idea behind
autodiff. It’s a class called array with some
operations such as addition of integer and
multiplication with other arrays.
It also has a method that returns the gradient called
grad. The gradient of the array is just the identity
function.
The trick is what happens with the addition and
multiplication. If you add something to an array or
you multiply two arrays, the result again is an array,
and has again a gradient. The product actually has
two gradients, one for each array involved.
The magic here is that while we are doing some
computation, we are keep track of that computation
and building up a graph of how to compute the
gradient of it.
---
.smaller[
```python
a = array(np.array([1, 2]), 'a')
b = array(np.array([3, 4]), 'b')
c = b * a
d = c + 1
print(d.value)
print(d.grad(1))
```]
```
[4 9]
{'b': array([1, 2]), 'a': array([3, 4])}
```
???
On the right you can see the result of adding two
arrays and then computing the gradient at a
particular location.
FIXME the gradient of multiplication?
Any computation in the neural network is a simple
operation like a matrix multiplication, addition or nonlinearity.
If we write down the derivative of each of
these, we can keep track of our computation and
automatically get the derivative.
It’s really easy to implement but really helpful.
Keep in mind that we actually hard-code the derivative
for each operation, there is no symbolic
differentiation involved.
---

---
# GPU Support

.center[
![:scale 45%](images/gpu_support.png)
]

- From http://www.nvidia.com/object/gpu-accelerated-applic
ations-tensorflow-benchmarks.html

- Take with a grain of salt.
???
An important limitation of GPUs is that they usually
have much less memory than the RAM, and memory
copies between RAM and GPU are somewhat
expensive.
---

#Computation Graph

.center[
![:scale 100%](images/computation_graph.png)
]

???
Autodiff doesn’t solve all of the problems, though.
Depending on which derivations you need to
compute, you need to store different intermediate
results (the net activation in backprob for example).
Given the limited amount of memory available, it’s
important to know what to store and what can be
discarded. That requires an end-to-end view of the
computation, which can be summarized as a graph.
Then you can use knowledge about the computation
graph to decide what results to cache, what results to
throw away and when to communicate between CPU
and GPU.
Having a representation of the graph also helps with
visual debugging and understanding your network
structure. The computation graph is more complex
than the network graph because it includes
intermediate results and gradient computations.
---
# All I want from a deep learning framework

- Autodiff

- GPU support

- Optimization and inspection of computation graph

- on-the-fly generation of the graph?

- distribution over cluster?

- Choices:
  
  – TensorFlow
  
  – Theano
  
  – Torch (lua)
???
So create deep learning model efficiently, I need
support for auto diff, computation on a GPGPU, and
optimization and inspection of the computation
graph.
There is some more advanced features that can come
in handy for research, like on-the-fly generation of
computation graphs, and distributing the network
over multiple GPUs or over a cluster.
At least the first three are all provided by the
TensorFlow, Theano and Torch frameworks, which
are the most established deep learning frameworks.
These don’t provide deep learning models, they are
infrastructure, more like numpy on steroids than
sklearn.
Theano was an early deep learning library for python,
but I think it has become less relevant since
tensorflow got released.
---
#Deep Learning Libraries

- tf.learn (Tensorflow)

- Keras (Tensorflow, Theano)

- Lasagna (Theano)

- Torch.nn / PyTorch (torch)

- Chainer (chainer)

- MXNet (MXNet)

- Also see:
http://mxnet.io/architecture/program_model.html


???
Then there are actual deep learning libraries that
provide higher level interfaces built on top of this
infrastructure.
There are a lot of these right now given the deep
learning hype, I want to point out a couple.
There is tf.learn, a high-level interface to tensorflow,
including a scikit-learn compatible API.
There’s keras, which supports both Tensorflow and
theano, there’s lasagna, which I think has seen less
activity recently, and torch.nn (lua) and pytorch
building on top of the torch framework.
Then there are two projects that are deep learning
libraries but that also come with their own framework
underneath, chainer and mxnet.
I think right now Keras with tensorflow is the most
commonly used one, and I want to go into those two
a bit more. But these all have their pros and cons.
---

#Quick look at TensorFlow

- “down to the metal” - don’t use for everyday tasks

- Three steps for learning:
  
  – Build the computation graph (using array operations
  and functions etc)
  
  – Create an Optimizer (gradient descent, adam, …)
  attached to the graph.
  
  – Run the actual computation.

???
---
.center[
![:scale 75%](images/tensor_flow_basics.png)
]

???
---
#A little more tf

- Everything passed to the graph is a “tensor”

- Either “variable”, “constant”, “placeholder”

- Learned parameters: variables

- Data: placeholders (data is assigned using “feed”)

???
---
# Great Resources!

- https://www.tensorflow.org/versions/r0.10/tutorials/

- http://playground.tensorflow.org

- https://www.tensorflow.org/versions/r0.10/get_star
ted/basic_usage

- https://www.tensorflow.org/versions/r0.10/tutorials
/mnist/beginners/

- Note: tf.learn is now officially tf.contrib.learn

- Tensorboard web interface

???
---
class:center,middle
#Introduction to Keras
???
---
#Keras Sequential

.smaller[
```python
from keras.models import Sequential
from keras.layers import Dense, Activation

model = Sequential([
    Dense(32, input_shape=(784,)),
    Activation('relu'),
    Dense(10),
    Activation('softmax'),
])
```
```
Using TensorFlow backend.
```
]

.smaller[
```python
model = Sequential()
model.add(Dense(32, input_dim=784))
model.add(Activation('relu'))
```
]
.smaller[
```python
model = Sequential([
    Dense(32, input_shape=(784,), activation='relu'),
    Dense(10, activation='softmax'),
])
```
]


???
There are two interfaces to keras, sequential and the
functional, but we’ll only discuss sequential.
Sequential is for feed-forward neural networks where
one layer follows the other. You specify the layers as
a list, similar to a sklearn pipeline.
Dense layers are just matrix multiplications. Here we
have a neural net with 32 hidden units for the mnist
dataset with 10 outputs. The hidden layer nonlinearity
is relu, the output if softmax for multi-class
classification.
You can also instantiate an empty sequential model
and then add steps to it.
For the first layer we need to specify the input shape
so the model knows the sizes of all the matrices. The
following layers can infer the sizes from the previous
layers.
---

.smaller[
```python
model.summary()
```
]

.center[
![:scale 70%](images/model_summary.png)
]

???
---
# Setting Optimizer

.center[
![:scale 80%](images/optimizer.png)
]

.smaller[
```python
model.compile("adam", "categorical_crossentropy", metrics=['accuracy'])
```
]

???
Compile method picks optimization procedure and
loss
---

# Training the model

.center[
![:scale 80%](images/training_model.png)
]

???
---
#Preparing MNIST data
.smaller[
```python
from keras.datasets import mnist
import keras
(X_train, y_train), (X_test, y_test) = mnist.load_data()
X_train = X_train.reshape(60000, 784)
X_test = X_test.reshape(10000, 784)
X_train = X_train.astype('float32')
X_test = X_test.astype('float32')
X_train /= 255
X_test /= 255
print(X_train.shape[0], 'train samples')
print(X_test.shape[0], 'test samples')

num_classes = 10
# convert class vectors to binary class matrices
y_train = keras.utils.to_categorical(y_train, num_classes)
y_test = keras.utils.to_categorical(y_test, num_classes)
```
```
60000 train samples
10000 test samples
```
]


???
---
# Fit Model
.smaller[
```python
model.fit(X_train, y_train, batch_size=128, epochs=10, verbose=1)
```
]

.center[
![:scale 80%](images/model_fit.png)
]

???
---
#Fit with Validation

.smaller[
```python
model.fit(X_train, y_train, batch_size=128, epochs=10, verbose=1, 
          validation_split=.1)
```
]

.center[
![:scale 90%](images/validation_fit.png)
]


???
---
#Evaluating on Test Set

.smaller[
```python
score = model.evaluate(X_test, y_test, verbose=0)
print("Test loss: {:.3f}".format(score[0]))
print("Test Accuracy: {:.3f}".format(score[1]))
```

```
Test loss: 0.120
Test Accuracy: 0.966
```
]

???
---

# Loggers and Callbacks

.smaller[
```python
model = Sequential([
    Dense(32, input_shape=(784,), activation='relu'),
    Dense(10, activation='softmax'),
])
model.compile("adam", "categorical_crossentropy", metrics=['accuracy'])
history_callback = model.fit(X_train, y_train, batch_size=128,
                             epochs=100, verbose=1, validation_split=.1)
```
]

.smaller[
```python
pd.DataFrame(history_callback.history).plot()
```
]

.center[
![:scale 40%](images/logger_callback_plot.png)
]

???
---
#Wrappers for sklearn

See https://keras.io/scikit-learn-api/

.smaller[
```python
from keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor
from sklearn.model_selection import GridSearchCV
def make_model(optimizer="adam", hidden_size=32):
    model = Sequential([
        Dense(hidden_size, input_shape=(784,)),
        Activation('relu'),
        Dense(10),
        Activation('softmax'),
    ])
    model.compile(optimizer=optimizer,loss="categorical_crossentropy",   
                  metrics=['accuracy'])
    return model
clf = KerasClassifier(make_model)
param_grid = {'epochs': [1, 5, 10],  # epochs is fit parameter, not in make_model!
              'hidden_size': [32, 64, 256]}
grid = GridSearchCV(clf, param_grid=param_grid, cv=5)
grid.fit(X_train, y_train)
```
]

???
Useful for grid-search.
You need to define a callable that returns a compiled
model.
You can search parameters that in Keras would be
passed to “fit” like the number of epochs.
---
.smaller[
```python
res = pd.DataFrame(grid.cv_results_)
res.pivot_table(index=["param_epochs", "param_hidden_size"],
                values=['mean_train_score', "mean_test_score"])
```
]

.center[
![:scale 70%](images/keras_api_results.png)
]



???
Training longer overfits more and more units overfit
more, but both also lead to better results.
We should probably train much longer actually.
Setting the number of epochs via cross-validation is a
bit silly since it means starting from scratch again
each time. Using early stopping would be better.
---


class: middle
# Questions ?


    </textarea>
    <script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

    <script>
    // Config Remark
    remark.macros['scale'] = function (percentage) {
        var url = this;
        return '<img src="' + url + '" style="width: ' + percentage + '" />';
    };
    config_remark = {
        highlightStyle: 'magula',
        highlightSpans: true,
        highlightLines: true,
        ratio: "16:9"
    };
      var slideshow = remark.create(config_remark);
    // Configure MathJax
    MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] /* removed 'code' entry*/
    }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i = 0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
    </script>
  </body>
</html>
