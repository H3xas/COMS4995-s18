The role of neural networks in ML has become increasingly important in recent years.
We will only go through some relatively simple variants, and discuss how to implement them.
I highly recommend you take a full course on neural networks, though I know they are a bit hard
to get into.

In particular we will only talk about supervised feed-forward neural networks, both the standard fully
connected variants and convolutional neural networks. We'll not discuss recurrent neural networks
or LSTMs or anything more complex.

???

For some reason, whenever anyone talks about neural networks, they also talk about history,
so here's a very brief history of neural nets.
While there was a big boom in use of neural nets, say in the last 9 years, many of the methods
are actually from the 1990s, so much older than random forests of SVMs.
There are two things that people say fundamentally changed the usefullness of neural nets:
more data, and computers fast enough to process that data, in particular using GPUs.
Actually, neural nets were kind of successful in the 1990s already, Yann LeCun's 
convolutional networks were used to reach checks automatically in ATMs.
That worked well because for digit recognition we had enough traning data becaus the variety
was "small" enough.
For natural images or audio, the variety in the data is much much larger, and so we need
much larger datasets to learn neural networks from those. And using these larger datasets
was not possible to collect in the 90, and not feasible to learn on without fast GPUs.
For other datasets, like tabular data as we saw it before in this class, which have been
the most popular domain of machine learning for a long time, there is often not enough
data available for neural nets to work, and so people were very skeptical of them for a long time.

Because neural networks were so successful recently, there has been a giant amount of research
around them, and that resulted in many important improvements like the use or relu units,
drop out, adam, batch normalization and FIXME residual networks. We'll talk about these improvements
today and next time as well.

The dramatic shift that we've seen is mostly due to the availability of data and processing power, though.

???

I want to give you a brief overview of what these models are for before we dive into more details.
These are non-linear models that can be used for classification and regression. They can also be
used for much more, but we'll not talk about this in this class. They are what's called
universal approximators, so they can model, i.e. overfit any function, like trees or RBF Kernel SVMs.
They work particularly well for very large datsets, usually with at least tens of thousands of samples,
or even millions of examples.
They require solving a non-convex optimization problem, so the solution is dependent on initialization
and the method of optimization you choose.
They are notoriously slow to traing compared to many other models, requiring GPUs.
There is alot of fast implementations of neural networks on GPUs out there, but scikit-learn doesn't
contain any. We'll use scikit-learn a little bit for illustration purposes, but you probably don't
want to use it on any real problem. We'll discuss some other libraries that you can use instead, though,
in particular tensorflow and keras. You can run these on your laptop, but to really get the benefit
of GPUs you need to have the right hardware, so you can either use a cluster or GPU cloud resources
or maybe your gaming PC with an Nvidia card.

State of the art models in computer vision and audio take days or even weeks to train, often on
multiple GPUs.
In a sense, neural networks are quite related to linear models, as we'll see in a bit, working
also with dot products. So similar to those, you need to preprocess your data to be continuous and scaled.
Neural networks, probably because of the non-convex optimization, are actually much more sensitive
to scaling than linear models in practice.

FIXME Move this slide after intro?

And finally, as I said earier, there are many many different variants of this. Neural networks,
or deep learning, is really an umbrella term that describes a large number of different architectures
and algorithms. The things that we'll talk about first are often called vanilla neural networks,
or multilayer perceptron.
And the zoo of neural network types is growing all the time, given the amount of research in that area.
The only other type we'll cover is convolutional neural nets, which we'll cover next time.
Other common types include recurrent neural networks like LSTMs, there's recursive neural networks,
GANs that are all the rage because they can generate nice faces and cats and photos from drawings,
neural turing machines that can learn algorithms, and FIXME of course deep reinforcement learning that
can play better go than any human.

???
Before I introduce you to neural networks, I want to introduce you to how people often draw
neural networks, but drawing a model that you already know, binary logistic regression, in the same way.
This drawing basically only encodes the prediction process, not the model building or fitting process.

Often networks are drawn as circles, which basically represent numbers.
So here, I drew a four-dimensional input vector, x[0] to x[3]. For each input, we drew a little circle
representing this number.
In binary logistic regression, the output is computed as the inner product of the input vector and a
weight vector w, or in other words a weighted sum of the x's, weighted by the ws.
So each of these arrows corresponds to weighting the input by an w_i and then they are all added up to the output y.
In logistic regression we usually compute the output y as the probability, given by the logistic
sigmoid of this weighted sum. This is not drawn but kind of implicit in the picture.
I also didn't draw the bias here, which is also added to the final sum.
So to summarize, circles mean numbers, and arrows mean products between what's on the origin of the arrow,
and a coefficient that's part of the model. And arrows into circles mean sums, and they often imply
some non-linear function like the logistic sigmoid.
???
So now let's look at what a neural network is. It's very similar to the logistic regression, only applied
several times.
So here I drew three inputs x, from which we compute weighted sums. But now there is not only a single output,
but we compute several intermediate outputs, the hidden units.
Each of the hidden units is a weighted sum of the inputs, using different weights.
Each hidden unit corresponds to an inner product with a weight vector, so all together they correspond
to multiplying the input by a matrix, here a 3 by 4 matrix.
We also add a bias for each of the hidden units, so a vector of dimension 4 for all of them.
Then, we basically repeat the process and compute again weighted sums of the hidden units, to arrive at the outputs.
Here this would correspond to multiplying by a 4 by 2 matrix, and adding a vector of dimension 2.
Then we could apply for example a logistic sigmoid or softmax to get a classification output.
FIXME why two outputs?!
So we basically do two matrix multiplications. If that was all, we could simplify this by just multiplying the matrices
together into a single matrix, and we would just have a linear model.
But the interesting part is that in the hidden layer, after we compute the weighted sum, we apply a non-linear
function, often called activation function or just nonlinearity.
That is what makes this whole function non-linear, and allows us to express much more interesting relationships.
You can think of this as doing logistic regression with learning non-linear basis functions.
The process is written here in formulas, we take the input x, multiply with a matrix W, add a bias b,
and apply a non-linearity f, to get h.
Then we multipy h by another matrix W', add a bias b' and apply a non-linearity g.
This looks a bit like the word2vec we had last time, though here it's really very important to have
the non-linear activation functions.
Wha we want to learn from data are the weights and biases, so in this case a 3x4 matrix, a vector of dim 4,
a 2x4 matrix and a vector of dim 2.
Each of these steps of computation is usually called a layer. Here we have an input layer, one hidden layer,
and an output layer. The hidden layer is called hidden because the computation is not actually part of the result,
it's just internal to the algorithm. Though I'm drawing all these things in a similar way, don't confuse
them with graphical models, as I drew them for latent dirichlet allocation.
All these nodes here are completely deterministic functions, and the graph just illustrates very simple computations.

???
We can have arbitrary many layers in a neural network, and more layers or more units allow to express more complex
functions. And the term deep-learning is referring to neural nets with many hidden layers.
This type of network is called a multilayer perceptron, basically because it has more than one layer of computation.

For the output, it functions similar to linear models. You can do binary classification with a single output,
you can do multi-class classification with a softmax like in multinomial logistic regression, or you can have
just a single output for regression.
All of the hidden layers usually have the same non-linear function f, but that's not usually the same as the output function.

???
The two standard choices for the non-linear activation function are shown here, either the tanh, which is the more
traditional choice, or the rectified linear unit or relu, which is more commonly used recently.
Tanh basically squashes everything between -inf and +inf to -1 and 1 and saturates towards the infinities.
The rectified linear unit just is constant zero for all negative numbers, and then the identity.
One of the reasons given for preferring the relu unit is that the gradient of tanh is very small in most places,
which makes it hard to optimize.
???

So how are we gonna learn these parameters, these weights and biases for all the layers?
If we did logistic regression, this would be relatively simple. We know it's a convex optimization problem,
to optimize the log-loss, so we can just run any optimizer, and they'll all come to the same result.

For the neural network it's a bit more tricky. We can still write down the objective.
So let's say we have a single hidden layer, then the hidden layer is x multiplied by W with
bias b added, and then a non-linearity f, and the output o(x) is another matrix multiplication,
another bias, and the output non-linearity like softmax. 

If we want to find the parameters W1,w2,b1,b2, we want to do empirical risk minimization,
so for classification we want to minimize the cross-entropy loss for classification of the outputs o(x)
given the ground thruth y_i over the training set. For regression we'd just use the square loss instead.

FIXME why W1/W2 here and before W and W'?
We could also add a regularizer, like an L2 penalty on the weights W if we wanted, though
that's not necessarily as important in neural networks as in linear models.
Generally, this is the same approach as for linear models, only the formula for the output is more complicated
now.

In particular this objective is not convex, so we can basically not hope to find the global optimum. But we can still
try to find "good enough" values for the parameters w and b by running an optimizer.
We could use a gradient based optimizer like gradient descent, or newton, or conjugate gradient of lbfgs
on this objective and this will yield a local optimum, but not necessarily a global one, and that's
basically the best we can do.

Because you have to care about this optimization more than for many other models I'll go into a bit more details
about how this works. Let me know if anything is unclear.

???
To run an optimizer, we do need to compute the gradients for all our parameters though, and that's a bit
non-obious. Luckily there's a simple algorithm to do that, called backpropagation, that is computationally
very simple. You probably heard the name backpropagation before, and often people make a big deal out of it,
but it is not actually a learning algorithm or anything like that, it's just a nice way to compute the gradients
in a neural network.
And back propagation is basically just a clever application of the chain rule for derivatives.
So let's say we want to get the gradients for the first weight matrix W1, so del o/del w1.
If you try to write this down directly from the formula for the network it's a bit gnarly, but using the
chain rule we can simplify this a bit.
Let's define net(X) as the first hidden layer before the non-linearity. Then we can apply the chain rule (twice) and we see that we can
write the gradient as a product of three terms, the input to the first layer, x, the gradient of the non-linearity f,
and the gradient of the layer after W1, namely h. So to compute the gradient of the first weight vector, we need
the activation/value of the layer before, and the derivative of the activation after.
FIXME backpropagation image?
FIXME need deriviative of loss, not output (unless square loss kinda)
When computing the predictions, we compute all the activations of the layers, and we get an error.
So we already have all these values.
So to compute the gradients, we can do a single sweep, a backward pass, from the output to the input,
computing the derivatives using the chain rule.

It's probably educational to go through the details of this once yourself, deriving this starting from the chain
rule. But you can also just look it up in the deep learning book I linked to.
I don't think it's gonna be instructive if I try to walk you through the algebra.

Anyone see a problem with this? In particular the gradient of the non-linearity?
???

So if you're a mathematician you might be slightly confused now. I showed you these non-linearities earlier, the relu and the tanh.
But the relu is not actually differentiable. it's differentiable everywhere but at zero. So formally, there is no gradient,
and we can't do gradient descent. We can do what's called "subgradient" descent, though, which only requires a subgradient. At every
point where there's a gradient, we just use the gradient, and if you hit the zero, you can actually pick any gradient that is "below the function" here;
any tangent direction is fine.
But in reality, you're never going to hit the zero, computing with floating point numbers, so we really don't have to worry about it.
And just to point this out again, backpropagation is not an optimization algorithm, it's a way to compute gradients.
We still need to think about what optimizer to use to actually find the minimum given the gradients.
So how could we do that? Gradient descent for example(FIXME gave answer earlier)
???
FIXME partial derivative signs!!!
So doing standard gradient descent, we would update a weight matrix W_i but using the old W_i and taking a gradient step,
so subtracting the gradient of the loss wrt the paramters, summed over the whole training set, times some learning rate.
The problem with this is that it's quite slow. Computing all these gradients means that we need to pass all the examples
forward through the network, make predictions, and then do a backward pass with backpropagation.
That's a lot of matrix multiplications to do a single gradient step, in particular given that we want to do this for
very large datasets.
So what we can do to speed this up is doing a stochastic approximation, as we already saw for linear models,
doing stochastic gradient descent aka online gradient descent. Here, you pick a sample at random,
compute the gradient just considering that sample, and then update the parameter.
So you update the weights much more often, but you have a much less stable estimate of the gradient.
In practice, we often just iterate through the data instead of picking a sample at random.
And as with linear models, this is much faster than doing full batches for large datasets.

FIXME first online then mini-batch?

However, it's less stable, and also it doesn't necessarily use the hardware in the best way.
So we can do a compromise in where we look at mini-batches of size k, usually something like 64 or 512.
So we look at k samples, compute the gradients, average them, and update the weights.
That allows us to update much more often than looking at the whole dataset, while still having a more stable
gradient, and better being able to use the parallel computing capabilities of modern CPUs and GPUs.
This is what's used in practice basically always.
The reason why this is faster is basically that doing a matrix-matrix multiplication is faster than
doing a bunch of matrix-vector operations.

In principle we could also be using smarter optimization methods, like second order methods or LBFGS,
but these are often not very effective on these large non-convex problems.
One, called levenberg-marquardt is actually a possibility, but it's not really used these days.

???
As with linear models, tuning the learning rate is a major issue here, so finding a good value of eta.

Actually, having a constant learning rate during the whole learning process is not good.
Instead, we want to make bigger steps in the beginning and smaller steps in the end, so that's what
people used for many years.
Even better is to have a different learning rate for each parameter that we're tuning, based on
past gradient steps.
There's many heuristics for that, and it's somewhat related to diagonal approximations of second
order methods. There's adagrad, adadelta, rmsprop and may more.
When I first started doing neural nets, I actually used something similar, Rprop, from the 90s.
Arguably the state of the art for these per-parameter adaptive learning rates is adam. It has
some magic numbers in it but it seems to work well in practice. Rmsprop is also used by some.

In addition to the speed of convergence, there's the addition issue that you will get different
solutions depending on how you pick the learning rates, as it's a non-convex problem.

While there is a whole lot of literature on this topic, it's really hard to do actual theory
about this, as it's fundamentally impossible to actually find a global optimum. So nearly
all of these strategies are heuristics, that have just proven well to work in practice.
???

So for large datasets, something like adam or rmsprop is probably a good idea.
However, if you have a very small datasets, you might just try an off-the-shelf solver like lbfgs.
Lbfgs basically uses an estimate of the hessian and is originally meant for convex problems, but it
also works reasonably well for non-convex problems.
And if you have time and nerve, you can try tuning a learning rate or learning rate schedule
yourself, and that can give better results than using one of the heurstics, but only after
putting in some effort. One other commonly used strategy is using a constant learning rate
for all weights, and half it once learning slows down.
These are basically the options that scikit-learn gives you, with the default being adam.
For some of the toy datasets we'll use lbfgs instead, which is more likely to give good results
if it's feasible to run a large number of iterations.
So let's start using these models now, for now with scikit-learn.

FIXME add transition slide.
???
So lets start with some toy data. Here's my favorite two moons dataset.